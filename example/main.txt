2.1 Molecular orbital based machine learning (MOB-ML)
The total energy of a given chemical system can be written as the sum of the Hartree-Fock and correlation energies. The correlation energy, \( E_{\text {corr }} \), can be further decomposed into pair energies, \( \epsilon_{i j} \), associated with occupied molecular orbitals (MO) \( i \) and \( j \), such that \( E_{\mathrm{corr}}=\sum_{i j} \epsilon_{i j} \). MOB-ML learns pair energies by \( \epsilon_{i j} \approx \epsilon^{\mathrm{ML}}\left(\boldsymbol{f}_{i j}\right) \), where the features \( \boldsymbol{f}_{i j} \) describe the interactions between the molecular orbitals.[13] Due to the different scales of the values, the diagonal pair energies \( \epsilon_{\mathrm{d}}=\left\{\epsilon_{i j} \mid i=j\right\} \) and the offdiagonal pair energies \( \epsilon_{\mathrm{o}}=\left\{\epsilon_{i j} \mid i \neq j\right\} \) are trained separately, effectively producing two different models.
2.2 Gaussian Processes (GP)
Gaussian Processes (GP) are non-parametric kernel-based machine learning methods that predict the probabilistic distribution of unobserved data. Given the observed data \( (X, y), X \in \mathbb{R}^{N \times d}, y \in \mathbb{R}^{N} \) with a Gaussian noise \( \sigma^{2} \in \mathbb{R} \) and a prior covariance function or kernel \( K: \mathbb{R}^{d} \times \mathbb{R}^{d} \rightarrow \mathbb{R} \), the prediction \( f\left(X^{\prime}\right) \) test points \( X^{\prime} \in \mathbb{R}^{M \times d} \) is a joint Gaussian distribution such that
\[
\mathbb{E}\left[f\left(X^{\prime}\right)\right]=K\left(X^{\prime}, X\right) \hat{K}^{-1} y, \quad \operatorname{Var}\left[f\left(X^{\prime}\right)\right]=K\left(X^{\prime}, X\right) \hat{K}^{-1} K\left(X, X^{\prime}\right),
\]
where \( \hat{K}=K(X, X)+\sigma^{2} I \). The hyperparameters, which include the Gaussian noise and kernel parameters, are learned in GP training by maximizing the log marginal likelihood
\[
L=-\frac{1}{2} y^{T} \hat{K}^{-1} y-\frac{1}{2} \log |\hat{K}|-\frac{N}{2} \log 2 \pi .
\]
The typical way to calculate the above quantities is the Cholesky decomposition, which has a time complexity of \( O\left(N^{3}\right) \) and a memory complexity of \( O\left(N^{2}\right) \). Such complexities limit the training size of GP-based models, such as MOB-ML, to around 50000 data points.
